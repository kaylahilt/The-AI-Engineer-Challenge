# üéØ Prompt Evaluation & A/B Testing Strategy

## ‚úÖ Current Implementation: Langfuse Native A/B Testing

We've successfully implemented **Langfuse native A/B testing** using their official approach:

| Feature | Implementation | Status |
|---------|----------------|--------|
| **A/B Testing** | Label-based prompt variants (`prod-a`, `prod-b`) | ‚úÖ **Implemented** |
| **Traffic Splitting** | Weighted random selection (90/10 split) | ‚úÖ **Active** |
| **Analytics** | Automatic in Langfuse dashboard | ‚úÖ **Monitoring** |
| **Tracing** | Built-in with `langfuse.openai` wrapper | ‚úÖ **Automatic** |

## üîß How Our A/B Testing Works

### **1. Prompt Variant Selection**
```python
# Weighted random selection between variants
selected_label = random.choices(["prod-a", "prod-b"], weights=[0.9, 0.1])[0]

# Fetch the appropriate prompt variant
prompt = langfuse.get_prompt("aethon-system-prompt", label=selected_label)
```

### **2. Automatic Tracing & Analytics**
```python
# Use Langfuse-wrapped OpenAI client for automatic tracing
from langfuse.openai import openai

response = openai.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "system", "content": prompt.prompt}],
    # Links prompt to generation for analytics
    langfuse_prompt=prompt,
    langfuse_metadata={"ab_test_variant": selected_label}
)
```

### **3. Dashboard Analytics**
- **Automatic metrics** comparison between variants
- **Response latency** and **token usage** tracking
- **Cost per request** by variant
- **Quality evaluation scores** (when configured)

## üìä Evaluation Methods Comparison

| Method | Pros | Cons | Cost | Accuracy | Speed |
|--------|------|------|------|----------|-------|
| **Langfuse Native** | Real user data, automatic analytics | Requires traffic, slower results | üí∞üí∞ Medium | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Highest | ‚ö° Slow |
| **LLM-as-Judge** | Semantic understanding, flexible | API costs, potential bias | üí∞üí∞ Medium | ‚≠ê‚≠ê‚≠ê‚≠ê High | ‚ö°‚ö° Medium |
| **Keyword Matching** | Fast, deterministic, free | Crude, misses semantics | üí∞ Free | ‚≠ê‚≠ê Low | ‚ö°‚ö°‚ö° Fast |
| **Semantic Similarity** | Captures meaning, consistent | Needs good references | üí∞üí∞ Medium | ‚≠ê‚≠ê‚≠ê Medium | ‚ö°‚ö° Medium |

## üéØ **CURRENT PRODUCTION STRATEGY**

### **Phase 1: Development Testing**
- **Primary**: Langfuse native evaluation with `test_prompt_quality.py`
- **Secondary**: Manual testing and validation
- **Goal**: Fast iteration and immediate feedback

### **Phase 2: A/B Testing (Production)**
- **Implementation**: Langfuse native A/B testing with `ABTestManager`
- **Traffic Split**: 90% control (`prod-a`) / 10% test (`prod-b`)
- **Analytics**: Automatic dashboard monitoring
- **Goal**: Real user impact measurement

### **Phase 3: Continuous Monitoring**
- **Metrics**: Langfuse dashboard analytics
- **Quality Gates**: Automated alerts for performance drops
- **Optimization**: Data-driven prompt improvements

## üöÄ Setting Up A/B Testing

### **Step 1: Create Prompt Variants**
```python
# Through the ABTestManager (already integrated)
from api.ab_testing import ABTestManager

ab_manager = ABTestManager()
# Variants are automatically managed through Langfuse labels
```

### **Step 2: Monitor Results**
1. **Langfuse Dashboard**: Compare metrics between `prod-a` and `prod-b`
2. **API Endpoint**: Check test status via `/api/ab-test/status/aethon-personality`
3. **Analytics**: Review latency, cost, and quality metrics

### **Step 3: Promote Winners**
Based on statistical significance, promote winning variants to full production.

## üîß Configuration Management

### **Environment Variables**
```bash
# Required for Langfuse integration
LANGFUSE_PUBLIC_KEY="pk_..."
LANGFUSE_SECRET_KEY="sk_..."
OPENAI_API_KEY="sk-..."

# Optional
LANGFUSE_HOST="https://cloud.langfuse.com"

# A/B Testing Configuration (New!)
AB_TESTING_ENABLED="false"  # Set to "true" to enable A/B testing
AB_TESTING_SPLIT="0.1"      # Percentage for test variant (0.0-1.0)
```

### **A/B Test Configuration**
```bash
# Configure via environment variables (recommended)
AB_TESTING_ENABLED="false"  # Currently disabled since we have one prompt
AB_TESTING_SPLIT="0.1"      # 10% test traffic when enabled

# To enable A/B testing:
# 1. Set AB_TESTING_ENABLED="true"
# 2. Set AB_TESTING_SPLIT="0.1" (or desired percentage)
# 3. Create prod-a and prod-b variants in Langfuse
# 4. Redeploy to Vercel
```

**Current Status**: A/B testing is **disabled** since we only have one prompt. When you're ready to test multiple variants:

1. **Create variants** in Langfuse with labels `prod-a` and `prod-b`
2. **Set environment variables** in Vercel dashboard
3. **Redeploy** to activate A/B testing

## üìä Quality Metrics & Monitoring

### **Automatic Metrics (Langfuse)**
- **Latency**: Response time by variant
- **Cost**: Token usage and API costs
- **Usage**: Request volume and patterns
- **Errors**: Failure rates and types

### **Custom Evaluations**
- **Personality Consistency**: Aethon character adherence
- **Response Quality**: Wisdom and practical advice
- **User Engagement**: Response satisfaction scores

## üö® **ROLLBACK PROCEDURES**

### **Immediate Issues**
1. **Disable A/B Testing**: Set traffic split to 100% control variant
2. **Check Logs**: Review Langfuse traces for error patterns
3. **Verify API Status**: Confirm OpenAI and Langfuse connectivity

### **Emergency Commands**
```bash
# Disable A/B testing (fallback to prod-a only)
curl -X POST "localhost:8000/api/ab-test/toggle/aethon-personality" \
  -H "Content-Type: application/json" \
  -d '{"enabled": false}'

# Check current status
curl "localhost:8000/api/ab-test/status/aethon-personality"
```

## üéØ **NEXT STEPS**

### **Immediate Actions**
1. ‚úÖ **A/B Testing Infrastructure** - Completed
2. üîÑ **Monitor Initial Results** - Collect baseline metrics
3. üîÑ **Set Up Alerts** - Configure performance thresholds

### **Short Term Goals**
4. **Custom Evaluators** - Add Aethon-specific quality metrics
5. **Statistical Analysis** - Implement significance testing
6. **Automated Decisions** - Auto-promote winning variants

### **Long Term Vision**
7. **Multi-variate Testing** - Test multiple prompt aspects
8. **Advanced Analytics** - Custom dashboards and reporting
9. **ML-Driven Optimization** - Automated prompt improvement

## üí° **KEY BENEFITS**

### **‚úÖ Production Ready**
- Battle-tested Langfuse infrastructure
- Automatic error handling and fallbacks
- Real-time monitoring and analytics

### **‚úÖ Developer Friendly**
- Simple integration with existing code
- Minimal configuration required
- Clear separation of concerns

### **‚úÖ Data Driven**
- Real user behavior insights
- Statistical significance testing
- Continuous improvement feedback loop

---

*This document reflects the current implementation as of the latest codebase updates. For technical implementation details, see the `api/ab_testing/` module.*